Форма для отзывов: https://forms.gle/kkcPJtXRsJKmKyNo8. Также, я буду благодарен за Merge Request-ы в этом репозитории.

# Seminar 2

# MPI

### Запуск программа на кластере

Вход на кластер

```[bash]
ssh <USER>@calc.cod.phystech.edu
```

[Гайд по работе с ssh](https://www.digitalocean.com/community/tutorials/ssh-essentials-working-with-ssh-servers-clients-and-keys)

Для работы с программами использующими MPI и OpenMP требуется подключить модуль (на научном кластере МФТИ иногда происходят изменения: одни модули исчезают, на их место приходят новые. Поэтому актуально на 2022 год):

```[bash]
module add centos/8/mpi/hpcx-v2.7.0
```

Посмотреть какие модули есть на кластере можно с помощью команды:

```[bash]
module avail
```

Для сборки mpi программы из исходников применяется специальный компилятор, являющийся обёрткой над gcc (хотя можно перенастроить и под clang). Запускаем так:

```[bash]
mpicc <путь_к_исходнику>
```

Или для c++ версии:

```[bash]
mpic++ <путь к исходнику>
```

Запускаем программу и убраем лишнее из вывода:

```[bash]
mpiexec ./a.out 2>/dev/null
```

Количество запущенных процессов оказалось равным количеству физических ядер компьютера. По умолчанию mpiexec запускается локально на всех возможных ядрах (можно проверить сколько их -nproc). Для ограничения этого можно запустить mpiexec -np <колчиество процессов>.

P.S. mpiexec и mpirun на практике работают одинаково.

### Небольшие итоги MPI:

#### Общее:

1. Хедер ```#include<mpi.h>```.
2. Внутри main ```прописываем MPI_Init(&argc, &argv)``` -- начало секции параллельной работы, после неё процессы становятся различимы. До этого это просто были n копий одной программы, работающие независимо.
3. ```MPI_Comm_size(MPI_Comm, int*)``` - запрашивает у коммуникатора количество процессов, подключённых к нему. В частности, в коде используется глобальный коммуникатор по умолчанию "MPI_COMM_WORLD".
4. ```MPI_Comm_rank(MPI_Comm, int*)``` - запрашивает у коммуникатора номер текущего процесса среди всех. Аналогично, в коде используем коммуникатор "MPI_COMM_WORLD".
5. ```MPI_Get_processor_name(char*, int*)``` - запрашивает название процессора текущей машины кластера и длинну этого названия.
6. ```MPI_Finalize``` - завершение параллельной части, открытой MPI_Init.

#### Передача сообщений между процессами

7. ```MPI_Send(...)``` - используется для отправки сообщений через выбранный коммуникатор выбранному получателю.
```[bash]
MPI_Send(
    void* /* указатель на начало данных */,
    int /* количество элементов */,
    MPI_INT/MPI_DOUBLE/... /* тип элементов (из списка стандартных MPI) */, 
    int /* номер получателя */, 
    int /* тэг сообщения */,
    MPI_COMM_WORLD /* коммуникатор */
)
```
8. ```MPI_Recv(...)``` - Используется для получения сообщений из выбранного коммуникатора от выбранного процесса. Параметры аналогичны Send за исключением MPI_Status - он используется для хранения данных об сообщении в случае, если мы получили его без указания получателя или подобного.
```[bash]
MPI_Recv(
    void* /* указатель на начало данных */, 
    int /* количество элементов */, 
    MPI_INT/MPI_DOUBLE/... /* тип элементов (из списка стандартных MPI) */, 
    0 /* номер отправителя */, 
    0 /* тэг сообщения */, 
    MPI_COMM_WORLD /* коммуникатор */,
    MPI_Status* /* данные о параметрах сообщения (откуда, сколько, с каким тэгом) */
)
```

#### Неопределённый источник сообщения, неопределённое количество.

9. Send и Recv принимают как параметр тег и адрес отправителя/получателя, чтобы это обойти есть: MPI_ANY_TAG и MPI_ANY_SOURCE -- специальный переменные, которые позволяют заменить тег и номер получателя на нечто неопределённое.
10. Чтобы понять от кого пришло сообщение существует переменная status. По сути это структура с таким содержимым:
* Status.MPI_SOURCE
* Status.MPI_TAG
11. `MPI_Get_count (MPI_Status*, MPI_Type, int*)` - получает из статуса сообщения требуемое количество элементов нужного типа и записывает его в последний аргумент. Однако тогда вопрос, как нам получить status до получения самого сообщения?

12. `MPI_Probe (int source, int tag, MPI_comm comm, *status )` - записывает данные о следующем сообщении с тегом tag от процесса source в переменную status. При этом само сообщение считается не прочитанным и его можно прочитать уже зная параметры. Или даже сделать ещё раз Probe (правда не понятно зачем).

#### Асинхронность 

13. ```ISend (array, size, MPI_Type, target number, tag, MPI_COMM_WORLD, &request)``` - асинхронный посылатель сообщений. Позволяет не ждать, пока второй поток получит сообщение, а просто работать дальше. Сообщение будет лежать в очереди на получение.
14. ```IRecv (array, size, MPI_Type, sender number, tag, MPI_COMM_WORLD, &request)``` - это асинхронный приниматель сообщений. Отличие от MPI_Recv -- request вместо status.
15. ```MPI_Request request``` - специальная переменная, содержащая 'ссылку' на асинхронный процесс отправки или получения.
16. ```MPI_Wait (&request, &status)``` - блокирующая функция ожидания, которая ожидает завершения события внутри request и при его завершении результат операции записывается в status
17. ```MPI_Test (&request, bool &flag, &status)``` - не блокирующая функция ожидания. Опрашивает, не пришло ли ещё сообщение, и результат записывает в переменную flag. Если сообщение пришло, то параметры сообщения записываются в status
18. ```MPI_Wtime()``` -  возвращает количество секунд, прошедшего с "некоторого" момента времени. С какого именно, решает компилятор. Известно только, что однажды зафиксировавшись, он остаётся неизменным для каждого потока.

* [пример 7-8](https://github.com/YHx07/pd-seminars/blob/main/seminar-01/code/01-send_recv/main.cpp)
* [пример 9](https://github.com/YHx07/pd-seminars/blob/main/seminar-01/code/05/main.cpp)
* [пример 11-12](https://github.com/YHx07/pd-seminars/blob/main/seminar-01/code/03-probe-message-status/probe.cpp)
* [пример 13-16](https://github.com/YHx07/pd-seminars/blob/main/seminar-01/code/04-isend-irecv/main.cpp)

[Можно ещё раз пересмотреть в другом источнике](https://gitlab.com/fpmi-atp/pd2022s-supplementary/chernetskiy/-/blob/main/Seminar_1_MPI.md)

# OpenMP

Хедер: ```#include<omp.h>```.

Директивы #pragma позволяют указать компилятору на области где возможно распараллеливание. В начале работы программы существует одна "основная" нить (thread). Последовательные участки программы выполняет основная нить. При входе в параллельную область создаются новые нити, которые уничтожаются при выходе из параллельной области.

Формат директивы на С/С++:

```[C]
#pragma omp directive-name [опция,..]
```

Все директивы OpenMP делятся на три 3 категории: определение параллельной области, распараллеливание, синхронизация.

Параллельная часть программы начинается с директивы:

```[C]
#pragma omp parallel [опция,..]
```

В OpenMP переменные в параллельных частях программы разделяются на два вида:

* Shared (общие)
* Private (локальные)

По умолчанию все переменные, которые входят в параллельную область ялвяются общими. Если, например используется локальный счетчик цикла, например *i*, то нужно указать:

```[C]
#pragma omp parallel shared (S), private(num_threads, myid, s, i)
```

#### Примеры секций:

`#pragma omp for`	-- Приводит к разделению работы, выполняемой в цикле for внутри параллельной области, между потоками.

`#pragma omp for nowait` -- Если какая-то нить отработала свою порцию работы параллельного цикла for, то она продолжит работу без ожидания других потоков.

`#pragma omp for  collapse(n)` -- Распараллеливание вложенных циклов. Для циклов образуется общий объем итераций, который равномерно распределеяется между потоками. Если опция не задана, то директива относится только к внешнему циклу.

`#pragma omp for collapse(n) nowait` -- Можно использовать несколько опций.

`#pragma  omp  critical` -- Указывает, что код выполняется только в одном потоке одновременно.

`#pragma omp barrier` -- Синхронизирует все потоки; все потоки приостанавливаются у барьера, пока все потоки не выполнят барьер.

[Взял отсюда](https://learn.microsoft.com/ru-ru/cpp/parallel/openmp/reference/openmp-directives?view=msvc-170#threadprivate)

#### Полезные функции:

1. `omp_set_num_threads(n)` -- установить n нитей
2. `omp_get_thread_num(n)` -- получить номер нити
3. `omp_get_wtime()` -- время

#### Запуск программы:

OpenMP можно запускать на своей машине, обычно все требуемые библиотеки уже стоят. Если нет, то см. первый семинар с запуском через Docker.

1. ```gcc -fopenmp main.c``` / ```g++ -fopenmp main.c```
2. ```./a.out```

#### Запуск программы через cmake:

1. ```cmake .```
2. ```make```
3. ```./a.out```

---

* [OpenMP specifications](http://www.openmp.org/specifications/)
* [OpenMP tutorial](https://www.openmp.org/resources/tutorials-articles/)
* [OpenMP more examples](https://github.com/ilyak/openmp-tutorial)
