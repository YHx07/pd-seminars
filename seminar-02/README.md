Форма для отзывов: https://forms.gle/kkcPJtXRsJKmKyNo8. Также, я буду благодарен за Merge Request-ы в этом репозитории.

# Seminar 2

На этом занятии:
1. Запустим программы с использованием MPI
2. Познакомимся и запуститм программы с использованием OpenMP

# MPI

### Напоминание запуск программ на кластере

#### Локальный запуск на клиентской машине:

Вход на кластер

```[bash]
ssh <USER>@185.81.248.52 -p 2223
```

[Гайд по работе с ssh](https://yandex.cloud/ru/docs/compute/operations/vm-connect/ssh)

Для сборки mpi программы из исходников применяется специальный компилятор, являющийся обёрткой над gcc. Запускаем так:

```[bash]
mpicc <путь_к_исходнику>
```

Или для c++ версии:

```[bash]
mpic++ <путь к исходнику>
```

Запускаем программу и убраем лишнее из вывода:

```[bash]
mpiexec ./a.out 2>/dev/null
```

#### Параллельный запуск с использованием нескольких машин кластера с помощью SLURM

Готовим файл run.sh:
```[bash]
#!/bin/bash
sbcast -f a.out a.out #Копирование a.out на остальные ноды для запуска
mpiexec ./a.out
```

Отправляем задачу в очередь на Slurm:
```[bash]
sbatch -n 4 --comment="Hello" run.sh
```

### Команды в MPI:

#### Общее:

1. Хедер ```#include<mpi.h>```.
2. Внутри main ```прописываем MPI_Init(&argc, &argv)``` -- начало секции параллельной работы, после неё процессы становятся различимы. До этого это просто были n копий одной программы, работающие независимо.
3. ```MPI_Comm_size(MPI_Comm, int*)``` - запрашивает у коммуникатора количество процессов, подключённых к нему. В частности, в коде используется глобальный коммуникатор по умолчанию "MPI_COMM_WORLD".
4. ```MPI_Comm_rank(MPI_Comm, int*)``` - запрашивает у коммуникатора номер текущего процесса среди всех. Аналогично, в коде используем коммуникатор "MPI_COMM_WORLD".
5. ```MPI_Get_processor_name(char*, int*)``` - запрашивает название процессора текущей машины кластера и длинну этого названия.
6. ```MPI_Finalize``` - завершение параллельной части, открытой MPI_Init.

#### Передача сообщений между процессами

7. ```MPI_Send(...)``` - используется для отправки сообщений через выбранный коммуникатор выбранному получателю.
```[bash]
MPI_Send(
    void* /* указатель на начало данных */,
    int /* количество элементов */,
    MPI_INT/MPI_DOUBLE/... /* тип элементов (из списка стандартных MPI) */, 
    int /* номер получателя */, 
    int /* тэг сообщения */,
    MPI_COMM_WORLD /* коммуникатор */
)
```
8. ```MPI_Recv(...)``` - Используется для получения сообщений из выбранного коммуникатора от выбранного процесса. Параметры аналогичны Send за исключением MPI_Status - он используется для хранения данных об сообщении в случае, если мы получили его без указания получателя или подобного.
```[bash]
MPI_Recv(
    void* /* указатель на начало данных */, 
    int /* количество элементов */, 
    MPI_INT/MPI_DOUBLE/... /* тип элементов (из списка стандартных MPI) */, 
    0 /* номер отправителя */, 
    0 /* тэг сообщения */, 
    MPI_COMM_WORLD /* коммуникатор */,
    MPI_Status* /* данные о параметрах сообщения (откуда, сколько, с каким тэгом) */
)
```

#### Неопределённый источник сообщения, неопределённое количество.

9. `MPI_Send` и `MPI_Recv` принимают как параметр тег и адрес отправителя/получателя, чтобы это обойти можно в параметрах указать `MPI_ANY_TAG` и `MPI_ANY_SOURCE` -- специальный переменные, которые позволяют заменить тег и номер получателя на произвольный тэг.
10. Чтобы понять от кого пришло сообщение существует переменная `status`. По сути это структура с таким содержимым:
* `status.MPI_SOURCE`
* `status.MPI_TAG`
11. `MPI_Get_count (MPI_Status*, MPI_Type, int*)` - получает из статуса сообщения требуемое количество элементов нужного типа и записывает его в последний аргумент.
12. `MPI_Probe (int source, int tag, MPI_comm comm, *status )` - записывает данные о следующем сообщении с тегом `tag` от процесса `source` в переменную `status`. При этом само сообщение считается не прочитанным и его можно прочитать уже зная параметры.

#### Асинхронность 

13. ```ISend (array, size, MPI_Type, target number, tag, MPI_COMM_WORLD, &request)``` - асинхронный посылатель сообщений. Позволяет не ждать, пока второй поток получит сообщение, а просто работать дальше. Сообщение будет лежать в очереди на получение.
14. ```IRecv (array, size, MPI_Type, sender number, tag, MPI_COMM_WORLD, &request)``` - это асинхронный приниматель сообщений. Отличие от `MPI_Recv` -- `request` вместо `status`.
15. ```MPI_Request request``` - специальная переменная, содержащая 'ссылку' на асинхронный процесс отправки или получения.
16. ```MPI_Wait (&request, &status)``` - блокирующая функция ожидания, которая ожидает завершения события внутри `request` и при его завершении результат операции записывается в `status`.
17. ```MPI_Test (&request, bool &flag, &status)``` - не блокирующая функция ожидания. Опрашивает, не пришло ли ещё сообщение, и результат записывает в переменную flag. Если сообщение пришло, то параметры сообщения записываются в status

#### Прочее

19. ```MPI_Wtime()``` -  возвращает количество секунд, прошедшего с "некоторого" момента времени. С какого именно, решает компилятор. Известно только, что однажды зафиксировавшись, он остаётся неизменным для каждого потока.

* [пример на команды 1-6](https://github.com/YHx07/pd-seminars/blob/main/seminar-01/code/00-hello-world/main.cpp)
* [пример на команды 7-8](https://github.com/YHx07/pd-seminars/blob/main/seminar-01/code/01-send_recv/main.cpp)
* [пример на команды 9](https://github.com/YHx07/pd-seminars/blob/main/seminar-01/code/05/main.cpp)
* [пример на команды 11-12](https://github.com/YHx07/pd-seminars/blob/main/seminar-01/code/03-probe-message-status/probe.cpp)
* [пример на команды 13-16](https://github.com/YHx07/pd-seminars/blob/main/seminar-01/code/04-isend-irecv/main.cpp)

Пример подсчёт времени работы `MPI_Wtime()`:
```c++
#include <mpi.h>
#include <iostream>
#include <unistd.h>

int calc (size_t num_of_elements)
{
    int res = 0;
    for (size_t i = 0 ; i < num_of_elements; i++)
    {
        res += (i*i) % 1000000000;
        res %= 1000000000;
    }
    return res;
}

int main(int argc, char** argv) {
    
    MPI_Init(&argc, &argv);

    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    int world_rank;

    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    
    size_t nums = 100000000;
    double start = 0;
    double end = 0;
    start = MPI_Wtime ();
    int res = calc (nums);
    end = MPI_Wtime ();

    std::cout << res << "  " << end - start << std::endl;

    MPI_Finalize();

    start = MPI_Wtime ();
    res = calc (nums);
    end = MPI_Wtime ();

    std::cout << res << "  " << end - start << std::endl;
    return 0;
}

```

[Можно ещё раз пересмотреть в другом источнике](https://gitlab.com/fpmi-atp/pd2022s-supplementary/chernetskiy/-/blob/main/Seminar_1_MPI.md)

# OpenMP

Хедер: ```#include<omp.h>```.

Директивы `#pragma` позволяют указать компилятору на области где возможно распараллеливание. В начале работы программы существует одна "основная" нить (thread). Последовательные участки программы выполняет основная нить. При входе в параллельную область создаются новые нити, которые уничтожаются при выходе из параллельной области.

Формат директивы на С/С++:

```[C]
#pragma omp directive-name [опция,..]
```

Например: `#pragma omp for nowait` -- параллельный фор без неявной синхронизации потоков в конце работы циклов. Соответсвенно `for` -- название директивы, `nowait` -- опция.

Все директивы OpenMP делятся на три 3 категории: определение параллельной области, распараллеливание, синхронизация.

Параллельная часть программы начинается с директивы:

```[C]
#pragma omp parallel [опция,..]
```

То есть до `#pragma omp parallel` программа выполнялась одной нитью, далее начинается работа `n` нитей.

В OpenMP переменные в параллельных частях программы разделяются на два вида:

* Shared (общие)
* Private (локальные)

По умолчанию все переменные, которые входят в параллельную область ялвяются общими. В задаче параллельного подсчета суммы используется локальный счетчик цикла, например *i*, то нужно указать:

```[C]
#pragma omp parallel shared (S), private(num_threads, myid, s, i)
```

Тут `S` -- общая переменная (сумма), `num_threads` -- количество нитей, `myid` -- идентификатор текущей нити, `s` -- локальная переменная для подсчета подсуммы, вычисленной на куске массива одной нитью, `i` -- локальная переменная для цикла.

#### Примеры секций:

`#pragma omp for`	-- Приводит к разделению работы, выполняемой в цикле for внутри параллельной области, между потоками.

`#pragma omp for nowait` -- Если какая-то нить отработала свою порцию работы параллельного цикла for, то она продолжит работу без ожидания других потоков.

`#pragma omp for collapse(n)` -- Распараллеливание вложенных циклов. Для циклов образуется общий объем итераций, который равномерно распределеяется между потоками. Если опция не задана, то директива относится только к внешнему циклу. Бывает полезен в случае вложенных циклов `for`. Пример (псевдокод):

```
for i in [1,2]:
    for j in [1,..,10]
```

В этом случае, если на машине имеем больше двух ядер, без `collapse` исполнение дойдет до первого цикла `for` и задействует только два потока, остальные потоки будут ждать.

`#pragma omp for collapse(n) nowait` -- Можно использовать несколько опций.

`#pragma omp critical` -- Указывает, что в этой секции работает только одна нить.

`#pragma omp barrier` -- Синхронизирует все потоки; все потоки приостанавливаются у барьера, пока все потоки не выполнят барьер.

#### Полезные функции:

1. `omp_set_num_threads(n)` -- установить n нитей
2. `omp_get_thread_num(n)` -- получить номер нити
3. `omp_get_wtime()` -- время

#### Запуск программы:

OpenMP можно запускать на своей машине, обычно все требуемые библиотеки уже стоят. Если нет, то см. первый семинар с запуском через Docker.

1. ```gcc -fopenmp main.c``` / ```g++ -fopenmp main.c```
2. ```./a.out```

#### Запуск программы через cmake:

Заходим папку с `CMakeLists.txt` файлом, далее:

1. ```cmake .```
2. ```make```
3. ```./a.out```

---

* [OpenMP specifications](http://www.openmp.org/specifications/)
* [OpenMP tutorial](https://www.openmp.org/resources/tutorials-articles/)
* [OpenMP more examples](https://github.com/ilyak/openmp-tutorial)
* [OpenMP Microsoft](https://learn.microsoft.com/ru-ru/cpp/parallel/openmp/reference/openmp-directives?view=msvc-170#threadprivate)
