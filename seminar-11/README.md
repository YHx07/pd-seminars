# Seminar 11

Все материалы есть тут: https://gitlab.com/fpmi-atp/pd2022a-supplementary/global/-/blob/main/materials/12-spark-rdd.md

# Spark

Apache Spark - платформа параллельной обработки данных для исполнения крупномасштабных аналитических приложений на кластерах. Спарк может обрабатывать как пакетные данные (batch), так и данные в реальном времени (stream). Напомню, что в курсе были Hadoop Streaming, Hive Streaming и Spark Streaming -- несмотря на "Streaming" каждый из них решает свою задачу. Фреймворд Spark написал на Scala, имеет Python API, который называется PySpark.

Spark Core, центр проекта, который обеспечивает распределенную передачу задач, планирование и функциональность ввода-вывода, предоставляет разработчикам потенциально более быструю и гибкую альтернативу MapReduce -  фреймворка, к которому были привязаны ранние версии Hadoop. Разработчики Spark говорят, что он может выполнять таски в 100 раз быстрее, чем MapReduce при обработке в памяти, и в 10 раз быстрее на диске.

Apache Spark может обрабатывать данные из различных хранилищ данных, включая HDFS, базы данных NoSQL и реляционные хранилища данных, такие как Apache Hive. Spark поддерживает обработку в памяти для повышения производительности приложений аналитики больших данных, но он также может выполнять обычную обработку на диске, когда наборы данных слишком велики, чтобы поместиться в доступную системную память.

Spark состоит из нескольких элементов:

- Spark SQL - для выполнения операций с данными, таких как традиционные задания SQL. Spark SQL предлагает API и SQL для управления данными.

- Spark Streaming и, в частности, Spark Structured Streaming для анализа потоковых данных. Унифицированный API Spark поможет обрабатывать данные аналогичным образом, независимо от того, являются ли они потоковыми или пакетными.

- Spark MLlib для машинного обучения и расширений в глубоком обучении.

- GraphX для использования графовых структур данных.

Spark - это отличный инструмент для дата инженеров, который можно использовать на всех этапах стандартного сценария работы с биг датой:

1) Загрузка данных (ingestion) - бронзовый слой данных (raw data)

2) Повышение качества данных (DQ) - серебряный слой данных (pure data)

3) Трансформация - золотой слой данных (rich data)

4) Публикация - загрузка данных в хранилище, использование BI-инструментов, вызов API или сохранение данных в файле.

Со Спарком можно работать на Java, Scala и Python (PySpark).

### Типичные задачи:

- Обучить модель на больших данных
- Проести ad-hoc анализ данных из двух таблиц

### Основные недостатки классического MapReduce:

- Постоянное чтение/запись во внешнее хранилище ⇒ не работает в реальном времени. Хотелось бы хранить сплиты в RAM.
- Сложный API. Нужно писать очень много кода
- Ограниченное число источников/приемников данных
- MapReduce — это только вычислительный фреймворк
- По сути один источник данных — диск (HDFS, локальная ФС клиента.. но все равно диск). Хотелось бы уметь читать и писать в другие источники (базы данных, облачные хранилища)

![image](https://user-images.githubusercontent.com/36137274/205483590-b603ed80-2d75-4bb7-ad6a-ba3bd15631b6.png)
https://cloud.mail.ru/public/VGM2/jdy1UABJQ

Планировщики:

- Standart Schedule (для запуска на одной машине)
- YARN
- Mesos
- Kubernetes

### Архитектура Apache Spark

![image](https://user-images.githubusercontent.com/36137274/205483642-0520b54c-d472-4fbb-9db7-a675944820e3.png)

https://cloud.mail.ru/public/1NaG/ujY4mYy2e

1. Driver program — управляющая программа
2. SparkContext — это основной объект, с его помощью взаимодействуем со Spark. SparkContext — всегда singleton объект.
3. Cluster manager — планировщик (один из..)
4. Executor — по сути JVM на нодах

В первом приближении работает также как в Hadoop. Единственное, контейнеры долго живущие. Контейнеры поднимаются один раз и умирают когда заканчивается SparkContext. Это позволяет хранить данные в памяти JVM. Быстрее RAM только кеши CPU, но это сложно реализуется (ассемблер).

### Spark RDD

Основной объект в Spark — RDD — Resilient Distributed Dataset. RDD — это набор данных, распределенный по партициям (аналог сплитов в Hadoop). Основной примитив работы в Spark.

Свойства:

- Неизменяемый. Можем получить либо новый RDD, либо plain object
- Итерируемый. Можем делать обход RDD. RDD — ..distributed.., он распределен по нодам кластера. Между частями можем ходить как по списку.
- Восстанавливаемый. Каждая партиция помнит как она была получена (часть графа вычислений) и при утере может быть восстановлена.

[Статья на DataBricks про Spark RDD](https://www.databricks.com/glossary/what-is-rdd). Материалы Databricks, наверное, один из самых лучших источников про работу на Spark.

RDD API состоит из операций двух типов:

- action
- transformation

Трансформация преобразовывает RDD в другой RDD и не приводит к вычислению графа (но не всегда, например sort)

Action заставляет Spark вычислить граф и вернуть результат либо на драйвер, либо во внешнее хранилище

Трансформации можно применять одну за другой, никаких вычислений не будет сделано, пока не будет вызван action.

Чем плох RDD:
1. Данные рассматриваются как текстовые файлы (или бинарники), разделенные строками. Про структуру данных ничего не знаем.
- Каждая задача начинается с парсинга структуры
- Иногда нужно работать со столбцами, а не со всей строкой
2. Неудобно. Хочется что-то похожее на SQL и / или pandas. "Заставляет аналитиков быть разработчиками"

Spark DataFrames:
- Похожи на DF в pandas или SQL
- Работают поверх RDD

Про Spark DataFrames смотри следующий семинар.

### PySpark Session

- Spark Context
- SQL Context
- Hive Context
- Streaming Context

Пример команды для запуска Spark Context:

```python
import os
import sys
os.environ["PYSPARK_PYTHON"]='/opt/anaconda/envs/bd9/bin/python'
os.environ["SPARK_HOME"]='/usr/hdp/current/spark2-client'
os.environ["PYSPARK_SUBMIT_ARGS"]='--num-executors 2 pyspark-shell'

spark_home = os.environ.get('SPARK_HOME', None)

sys.path.insert(0, os.path.join(spark_home, 'python'))
sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))

from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

conf = SparkConf()
conf.set('spark.app.name', 'Name')

spark = SparkSession.builder.config(conf=conf).appName('Name').getOrCreate()
```

В нашем случае, когда мы запускаем команду на кластере
```bash
PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_PYTHON=/usr/bin/python3 PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip="*" --port=<PORT> --no-browser' pyspark2 --master=yarn --num-executors=<N>
```
Spark Context выделяется вместе с поднятием юпитер ноутбука.

### YARN UI

На нашем кластере UI настроен не в полном объеме. Подробнее про YARN UI можно посмотрть тут (но для курса это не нужно):

https://github.com/YHx07/pd-seminars/blob/dev/seminar-11/yarn-ui.md
