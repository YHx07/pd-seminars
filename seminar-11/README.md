# Seminar 11

Все материалы есть тут: https://gitlab.com/fpmi-atp/pd2022a-supplementary/global/-/blob/main/materials/12-spark-rdd.md

# Как запустить Jupyter для запуска программ на PySpark:

В [материалах](https://gitlab.com/fpmi-atp/pd2022a-supplementary/global/-/blob/main/materials/12-spark-rdd.md) утверждаетя, что порты 30000..30100 нашего Hadoop кластера открыты наружу. Если так, то
1. Заходим по shh на кластер под своим логином,
2. Копируем из `/home/velkerr/seminars/pd2020/14-15-spark/05-spark-base_nb.ipynb` и `/home/velkerr/seminars/pd2020/14-15-spark/images` файлы к себе в директорию (можно без них, но с ними проще начать писать),
3. Запускам команду в терминале (выбираем себе порт <PORT> (надо чтобы он не совпадал с кем-то другим; если что-то не работает, то попробуйте подобрать другой порт) и на месте количества экзекьютеров <N> пишем, например, 3; далее прям так пишем в терминале): `PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_PYTHON=/usr/bin/python3 PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip="*" --port=<PORT> --no-browser' pyspark2 --master=yarn --num-executors=<N>`
4. В браузере открываем http://localhost:<PORT> и вставяем токен (он напишется в терминале, когда запускаем Jupyter)

У меня порт 30076 не работал, поэтому я его все-таки пробросил при подключении к кластеру:

<img width="1046" alt="Screenshot 2023-11-25 at 10 41 35" src="https://github.com/YHx07/pd-seminars/assets/36137274/30f7f9c9-b7dd-4dd7-a3a0-629225ca15ff">

# Spark

Apache Spark - платформа параллельной обработки данных для исполнения крупномасштабных аналитических приложений на кластерах. Спарк может обрабатывать как пакетные данные (batch), так и данные в реальном времени (stream). Напомню, что в курсе были Hadoop Streaming, Hive Streaming и Spark Streaming -- несмотря на "Streaming" каждый из них решает свою задачу. Фреймворд Spark написал на Scala, имеет Python API, который называется PySpark.

Spark Core, центр проекта, который обеспечивает распределенную передачу задач, планирование и функциональность ввода-вывода, предоставляет разработчикам потенциально более быструю и гибкую альтернативу MapReduce -  фреймворка, к которому были привязаны ранние версии Hadoop. Разработчики Spark говорят, что он может выполнять таски в 100 раз быстрее, чем MapReduce при обработке в памяти, и в 10 раз быстрее на диске.

Apache Spark может обрабатывать данные из различных хранилищ данных, включая HDFS, базы данных NoSQL и реляционные хранилища данных, такие как Apache Hive. Spark поддерживает обработку в памяти для повышения производительности приложений аналитики больших данных, но он также может выполнять обычную обработку на диске, когда наборы данных слишком велики, чтобы поместиться в доступную системную память.

Spark состоит из нескольких элементов:

- Spark SQL - для выполнения операций с данными, таких как традиционные задания SQL. Spark SQL предлагает API и SQL для управления данными.

- Spark Streaming и, в частности, Spark Structured Streaming для анализа потоковых данных. Унифицированный API Spark поможет обрабатывать данные аналогичным образом, независимо от того, являются ли они потоковыми или пакетными.

- Spark MLlib для машинного обучения и расширений в глубоком обучении.

- GraphX для использования графовых структур данных.

Spark - это отличный инструмент для дата инженеров, который можно использовать на всех этапах стандартного сценария работы с биг датой:

1) Загрузка данных (ingestion) - бронзовый слой данных (raw data)

2) Повышение качества данных (DQ) - серебряный слой данных (pure data)

3) Трансформация - золотой слой данных (rich data)

4) Публикация - загрузка данных в хранилище, использование BI-инструментов, вызов API или сохранение данных в файле.

Со Спарком можно работать на Java, Scala и Python (PySpark).

### Типичные задачи:

- Обучить модель на больших данных
- Проести ad-hoc анализ данных из двух таблиц

### Основные недостатки классического MapReduce:

- Постоянное чтение/запись во внешнее хранилище ⇒ не работает в реальном времени. Хотелось бы хранить сплиты в RAM.
- Сложный API. Нужно писать очень много кода
- Ограниченное число источников/приемников данных
- MapReduce — это только вычислительный фреймворк
- По сути один источник данных — диск (HDFS, локальная ФС клиента.. но все равно диск). Хотелось бы уметь читать и писать в другие источники (базы данных, облачные хранилища)

![image](https://user-images.githubusercontent.com/36137274/205483590-b603ed80-2d75-4bb7-ad6a-ba3bd15631b6.png)
https://cloud.mail.ru/public/VGM2/jdy1UABJQ

Планировщики:

- Standart Schedule (для запуска на одной машине)
- YARN
- Mesos
- Kubernetes

### Архитектура Apache Spark

![image](https://user-images.githubusercontent.com/36137274/205483642-0520b54c-d472-4fbb-9db7-a675944820e3.png)

https://cloud.mail.ru/public/1NaG/ujY4mYy2e

1. Driver program — управляющая программа
2. SparkContext — это основной объект, с его помощью взаимодействуем со Spark. SparkContext — всегда singleton объект.
3. Cluster manager — планировщик (один из..)
4. Executor — по сути JVM на нодах

В первом приближении работает также как в Hadoop. Единственное, контейнеры долго живущие. Контейнеры поднимаются один раз и умирают когда заканчивается SparkContext. Это позволяет хранить данные в памяти JVM. Быстрее RAM только кеши CPU, но это сложно реализуется (ассемблер).

### Spark RDD

Основной объект в Spark — RDD — Resilient Distributed Dataset. RDD — это набор данных, распределенный по партициям (аналог сплитов в Hadoop). Основной примитив работы в Spark.

Свойства:

- Неизменяемый. Можем получить либо новый RDD, либо plain object
- Итерируемый. Можем делать обход RDD. RDD — ..distributed.., он распределен по нодам кластера. Между частями можем ходить как по списку.
- Восстанавливаемый. Каждая партиция помнит как она была получена (часть графа вычислений) и при утере может быть восстановлена.

[Статья на DataBricks про Spark RDD](https://www.databricks.com/glossary/what-is-rdd). Материалы Databricks, наверное, один из самых лучших источников про работу на Spark.

RDD API состоит из операций двух типов:

- action
- transformation

Трансформация преобразовывает RDD в другой RDD и не приводит к вычислению графа (но не всегда, например sort)

Action заставляет Spark вычислить граф и вернуть результат либо на драйвер, либо во внешнее хранилище

Трансформации можно применять одну за другой, никаких вычислений не будет сделано, пока не будет вызван action.

Чем плох RDD:
1. Данные рассматриваются как текстовые файлы (или бинарники), разделенные строками. Про структуру данных ничего не знаем.
- Каждая задача начинается с парсинга структуры
- Иногда нужно работать со столбцами, а не со всей строкой
2. Неудобно. Хочется что-то похожее на SQL и / или pandas. "Заставляет аналитиков быть разработчиками"

Spark DataFrames:
- Похожи на DF в pandas или SQL
- Работают поверх RDD

Про Spark DataFrames смотри следующий семинар.

### PySpark Session

- Spark Context
- SQL Context
- Hive Context
- Streaming Context

Пример команды для запуска Spark Context:

```python
import os
import sys
os.environ["PYSPARK_PYTHON"]='/opt/anaconda/envs/bd9/bin/python'
os.environ["SPARK_HOME"]='/usr/hdp/current/spark2-client'
os.environ["PYSPARK_SUBMIT_ARGS"]='--num-executors 2 pyspark-shell'

spark_home = os.environ.get('SPARK_HOME', None)

sys.path.insert(0, os.path.join(spark_home, 'python'))
sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))

from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

conf = SparkConf()
conf.set('spark.app.name', 'Name')

spark = SparkSession.builder.config(conf=conf).appName('Name').getOrCreate()
```

В нашем случае, когда мы запускаем команду на кластере
```bash
PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_PYTHON=/usr/bin/python3 PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip="*" --port=<PORT> --no-browser' pyspark2 --master=yarn --num-executors=<N>
```
Spark Context выделяется вместе с поднятием юпитер ноутбука.

Про параметры spark context есть отличная статья от Авито: https://habr.com/ru/companies/avito/articles/732870/ .

### YARN UI

На нашем кластере UI настроен не в полном объеме, но большая часть есть (applications, job-ы и stage-ы).  Подробнее про YARN UI можно посмотрть тут:

https://github.com/YHx07/pd-seminars/blob/dev/seminar-11/yarn-ui.md

### Персистентность и кэширование

При перезапуске Action, пересчитывается весь граф вычислений. Это логично т.к. в трансформациях ничего не вычисляется. Полезно это тем, что если за время работы задачи данные обновились (дополнились), нам достаточно просто перевызвать Action. Но если данные не меняются (например, при отладке), такой пересчёт даёт Overhead. Можно **закешировать** часть pipeline. Тогда при след. вызове Action, RDD считается с кеша и пересчёт начнётся с того места, где было кеширование. В History UI все Stage перед этим будут помечены "Skipped".

Можно кэшировать RDD, чтобы его потом переиспользовать. RDD вычисляются лениво, когда вызывается action. Часто мы хотим вызывать несколько actions для одного и того же RDD. Если мы просто сделаем это, то граф будет полностью перевычислятсья каждый раз. Чтобы избежать этого, мы можем закешировать RDD в памяти. Кэширование произойдет при вызове первого action.

```python
rdd.cache()
```

В `persist()` можно указать [StorageLevel](https://spark.apache.org/docs/2.1.2/api/python/_modules/pyspark/storagelevel.html), т.е. на какой носитель кешируем. Можем закешировать в диск, в память, на диск и / или память на несколько нод... или дать возможность Spark'у решить самому (на основе объёма кеша).`cache()` - это простой вариант `persist()`, когда кешируем только в RAM.

Есть несколько уровней кэширования:

- Memory_only
- Memory_and_disk
- Disk_only
- Memory_only_2, 2 — это число дупликаций, на случай, если что-то упадет
- Memory_and_disk_2, 2 — это число дупликаций, на случай, если что-то упадет

```python
rdd.persist(storage_level)
```

Удаление данных из кэша:
```python
rdd.unpersist()
```

### Обработка отказов

- Промежуточные результаты не сохраняются в HDFS
- RDD по умолчанию не реплицируются
- Для восстановления потерянных данных используется информация о происхождении
- Spark восстанавливет потерянные фрагменты RDD, заново вычисляя их путем применения трансформаций


### Broadcast

Broadcast объект — это неизменяемая переменная, которая разделяется между всеми экзекьюторами. Дистрибуция broadcast-объекта производится быстро и эффективно p2p протоколом.

*Map side join*

Аналог DistributedCache в Hadoop. Обычно используется когда мы хотим в спарке сделать Map-side join (т.е. имеется 2 датасета: 1 маленький, который и добавляем в broadcast, другой большой).
