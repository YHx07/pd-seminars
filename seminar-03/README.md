# Seminar 3

# CUDA

CUDA -- Compute Unified Device Architecture. Мы будет использовать CUDA Toolkit для ускорения вычислений программ, написанных на C++, на видеокартах Nvidia. Инструкция на [официальном сайте Nvidia](https://developer.nvidia.com/how-to-cuda-c-cpp).

## GPU

GPU -- Graphics Processing Unit -- видеокарта.

Изначально использовалось для графики (рендеринг, обратная трассировка лучей).

Идея: вектор содержит много чисел и все элементры вектора подвергаются одной операции. Видеокарта позволит произвести эти однотипные вычисления за один такт процессора.

CUDA Toolkit можно использовать и для вычислений на TPU -- Tensor Processing Unit. Почитать про сравнение CPU, GPU, TPU можно [на cloud.google](https://cloud.google.com/tpu/docs/intro-to-tpu), или [на Habr-е на русском](https://habr.com/ru/post/422317/), или посмотреть [интерактивную презентацию Google](https://storage.googleapis.com/nexttpu/index.html).

## Кластер с GPU

Смотри инструкцию GPU-сервер АТП. Некоторые характеристики кластера:

RAM: 256 Gb,
CPU: Intel Xeon Gold 6136 v4, 24 ядра,
GPU: Nvidia GeForce RTX 2080, 8 видеокарт, видеопамять на каждой 11 Gb.

~ 3 Tb HDD и ~500 Gb SSD.

На кластере уставновлена CUDA 10.1.

- nvidia-smi

Первые шаги на кластере:

1. [Создать](https://www.ssh.com/academy/ssh/keygen) себе ssh ключ,
2. Добавить .ssh/id_rsa.pub к себе на гитхаб в ключи: https://github.com/settings/keys
3. git clone https://github.com/akhtyamovpavel/ParallelComputationExamples

[Небольшой лайфхак](https://www.ssh.com/academy/ssh/copy-id) как входить на кластер и не вводить каждый раз пароль.

## Программирование на CUDA

thread id, потоки управляются threadpool-ом (на самом деле не совсем, правильно - warp) -- это единое устройство управления потоками. Каждый warp имеет набор регистров (стандартно 32 -- количество потоков) и команду, применяет инструкцию

streaming multiprocessor (SM) -- аналог физических ядер

Пример с лекции: Например всего на видеокарте 2560 ядер и 20 SM => один SM управляет 128 ядрами. Один warp состоит из 32 потоков, поэтому один SM управляет 4 warp-ами.

Логическая абстракция 

- warp -- набор потоков, физически отрабатываемых за один такт времени.
- block -- набор потоков, логически отрабатываемых в одну единицу времени, block-и делятся на warp-ы в физическом исполнении. Программируя мы имеем дело с block-ами, с точки зрения кода warp-ы не видим.
- grid -- набор из блоков, количество блоков в grid-е задается пользователем.

Один блок обрабатывается одним SM.

Спецификация видеокарты

## Запуск программы на CUDA

### 00-hello-world-single-thread

Программа на C++, пока без CUDA. Тут складываем два вектора x и y: y = x + y, складываем их на одном ядре

Скомпилируем и замерим время:

```
1. g++ main.cpp 
2. time ./a.out
```

Видим, что максимальная ошибка равна нулю. Запомним время -- у меня получилось 3.6сек.

### 01-hello-world-single-cuda-thread

В чём отличие между программами? Первое -- main.cpp и main.cu, теперь используем CUDA и имеем другой синтаксис:

1. __global__ -- специальный макрос, который показывает, что функция может запускаться как на GPU, так и на CPU. 

Конструкция называется ядром:
```
__global__
void add
```

2. Память выделяется с помощью cudaMallocManaged (есть еще просто cudaMalloc, но с ним сложнее работать. cudaMallocManaged создает массив сразу и на CPU, и на GPU).

3. Появилось `<<<1, 1>>>` -- это гиперпараметры вызова ядра -- `__global__void add` из начала кода. То есть в `<<<..>>>` указываем параметры вызова ядра, а в `()` как обычно параметры вызова функции. То что мы указали явно 1,1 приведет к исполнению программы на видеокарте. Первая единица указывает, что используем один grid, второй что используем один блок. Один блок обрабатвается одним ядром видеокарты. По сути именно на моменте `add<<<1, 1>>>(N, x, y)` начинается работа с CUDA -- массивы копируются на видеокарту и затем складываются на ней.

4. cudaDeviceSynchronize() -- по сути это барьер. Так как работаем между GPU и CPU и так как все операции на GPU асинхронные, то чтобы считать содержимое массивов нам нужно поставить барьер, и убедиться что когда посчитался массив, он перекинулся на CPU.
 
Скомпилируем и замерим время (используем компилятор nvcc -- обертка над gcc):

```
1. nvcc main.cu
2. time ./a.out
```

Стало работать медленнее -- у меня 42.8cек. 

Есть утилита профилировщик:

```
3. nvprof ./a.out 
```

### 02-add-threads


### 03-add-blocks

Добавим еще одну абстракцию -- количество блоков в grid: numBlocks = (N + blockSize - 1) / blockSize. Раньше на этом месте стояла 1, поэтому в единицу времени обрабатывался 1 блок. Теперь в одну единицу времени можем обрабатывать столько блоков, сколько может видеокарта. 

Появляется переменная blockIdx -- индекс используемого блока. По сути говорим, что будем грузить пачками, надо посчитать номер ячейки.

### last

max threads per blocks -- максимальное значение, которое можно передавать во второй параметр <<<>>>

Можем посмотреть что всего у ядер у видеокарты RTX 2080 TI: 4352. Mupltiprocessor count: 68 => 4352 / 68 = 64 подъядра в каждом мультипроцессоре.  

---

### Материалы:

- [CUDA в примерах (рус)](https://cloud.mail.ru/public/DCPf/aCk7BnMTJ)
- [Developer.nvidia.com](https://developer.nvidia.com/accelerated-computing-training)
- [Sample Code](https://github.com/nvidia/cuda-samples)
