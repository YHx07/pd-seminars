Форма для отзывов: https://forms.gle/kkcPJtXRsJKmKyNo8. Также, я буду благодарен за Merge Request-ы в этом репозитории.

# Seminar 1 

В этом семинаре:
1. Интро в курс параллелок МФТИ
2. Как запускать код на MPI на кластере и что происходит
3. Альтернативный вариант, чтобы тестировать программу без использования кластера

Ещё один конспект: https://gitlab.atp-fivt.org/courses-public/pd/global/-/blob/main/materials/01-MPI.md

Информация о курсе: http://wiki.atp-fivt.org/index.php/Параллельные_и_распределенные_вычисления_весна_2025

### Параллельные и распределенные вычислительные системы

__Параллельные__:
* многократное ускорение
* высокопроизводительные машины
* отказоустойчивости нет
* с точки зрения разработчика -- набор взаимодействующих процессов

__Распределенные__:
* большие объёмы данных
* обычные машины
* отказоустойчивость
* с точки зрения разработчика -- одно распределенное вычислительное устройство

### О курсе
Курс состоит из трёх частей:
1. MPI/OpenMP -- параллельные вычисления на CPU (обычный процессор). 2 семинара
3. Cuda -- параллельные вычисления на GPU (видеокарта). 3 семинара
4. Hadoop -- распределенные вычисления (много CPU). 6 семинаров
   1. HDFS -- файловая система
   2. MapReduce -- парадигма на основе которой работают системы (такие как Hive, Spark) Hadoop
   3. Hive -- система для запуска SQL запросов на hadoop кластере (превращает SQL запрос в последовательность MapReduce задач)
   4. Spark -- система для обработки данных, более продвинутая чем  просто перевод  SQL в MapReduce ([подробнее тут](https://github.com/YHx07/pd-seminars/tree/main/seminar-11)). Код можно писать на Java, Scala и Python (PySpark). В случае PySpark программа выглядит как обычный код в Jupyter Notebookю

Части курса 1,2,3 между собой не связаны.

### MPI

MPI - Message Passing Interface. То есть по факту это API, созданный для передачи сообщений между процессами. Причём, процессами не только одной машины, но и распределённого кластера (множество машин в одной сети, воспринимаемые как одна вычислительная единица). У нас как раз будет и кластер (подключимся по ssh), и  одна машина (локальный запуск внутри Docker). Более того, программы, за счёт высокоуровневой абстракции, написанные с использование этого API, являются платформо-независимыми (на сколько позволяет сам исходный код программы).

С MPI программа разбивается на несколько частей, мы же рассмотрим для простоты 2 части:

#### Локальная
В данной части выполняется код, который будет одинаковым для всех запущенных параллельно процессов. То есть если вы запустили 10 процессов, и попросили их напечатать "Hello", то в консоли и получите 10 "Hello", причём эти процессы в данной части не различимы.

#### Параллельная
Начинается она с команды MPI_Init. После неё мы уже можем определить, какой номер у нашего процесса, куда ему стучаться и какую часть работы выполнять. Неформально - мы получаем доступ к ряду функций, которые позволяют определить уникальный номер нашего процесса, а так же позволяют отправлять и принимать данные между процессами.

### MPI vs Hadoop на распределенном кластере

MPI:
- MPI применяется для разработки программ, которые запускаются на множестве узлов кластера и обмениваются данными через сеть.
- Ориентирован на распределение вычислительных задач, которые могут требовать частого обмена данными между процессами на различных узлах.

Примеры использования: 
1. Решить уравненеие теплопроводности и укорить его решение с помощью паралельного алгоритма. Эта задача хорошо ложится в применение MPI, так как соседним потоком нужно общаться друг с другом, чтобы получать значение у соседей.
2. У вас есть распределенный кластер, в каждом узле GPU. Вы хотите обучить ML модель. Так как память видеокарты ограничена, может не получиться загрузить весь датасет в память видеокарты. Тогда вы можете разбить датасет на части, обучать в каждом узле на части данных. Затем организовать обмен данными между узлами с помощью MPI.

Hadoop:
- Применяет модель MapReduce, где задачи разделяются на стадии 'map' и 'reduce', которые легко распараллеливаются и масштабируются на большом количестве узлов.
- HDFS позволяет хранить большие объемы данных, распределенные по узлам кластера.

Примеры использования: 
1. У вас есть поисковые логи за вчера, в них сто миллиардов записей. Вам нужно преобразовать их с помощью регулярного выражения.

### MPI программа на С/С++

Реализация MPI-функций лежит в заголовочном файле ```mpi.h```.
Зона параллельной части программы находится между вызовами функций ```MPI_Init``` и ```MPI_Finalize```.
Чтобы получить количество процессов, используется ```MPI_Comm_size```. Определить id процесса среди N запущенных 
можно через ```MPI_Comm_rank```.

![MPI Reference](pic/mpi_reference.png)

Пример программы, выполняющей ```"Hello, World!"``` на каждом процессе и считающей количество процессов:

```
#include <stdio.h>
#include <mpi.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    
    int procid, num_procs;
    MPI_Comm_rank(MPI_COMM_WORLD, &procid);
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
    
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_length;
    MPI_Get_processor_name(processor_name, &name_length);
    
    printf("Hello, World! My id is %d and my processor name is %s\n", procid, processor_name);
    
    if (procid == 0) {
        printf("All processes count: %d\n", num_procs);
    }
    
    MPI_Finalize();
    return 0;
}
```

Пример на C++:

```
#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int procid, num_procs;
    MPI_Comm_rank(MPI_COMM_WORLD, &procid);
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
    
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_length;
    MPI_Get_processor_name(processor_name, &name_length);

    std::cout << "Hello, world! My id is " << procid << " and my processor name is " << processor_name << " out of " << world_size << std::endl;
	
    MPI_Finalize();
    return 0;
}
```

Разбор кода посмотрим в https://gitlab.atp-fivt.org/courses-public/pd/global/-/blob/main/materials/01-MPI.md?ref_type=heads#разбор-hello_mpi

### Компиляция MPI программы на С/С++

Компиляция программ происходит при помощи компиляторов ```mpicc``` и ```mpic++```.

Для программ на C:

```[bash]
mpicc <FILE_NAME>.c
```

Для программ на C++:

```[bash]
mpic++ <FILE_NAME>.cpp
```

### Вход на кластер

Вам на почту должно придти письмо от automation@atp-fivt.org с именем пользователя и паролем. Если не пришлом, смотрим папку спам. Если и там нет, то заполняем форму на регистрацию текущего года (2025 весна: https://docs.google.com/forms/d/e/1FAIpQLSdiXebJ4ddB7RybaVIPvsGiDoHmFBMNth2vIrhuXSlztsfDvg/viewform).

Вводим в терминале:
```[bash]
ssh <USER>@185.81.248.52 -p 2223
```

Кластер -- это несколько серверов, связанных сетью. При подключении по ssh мы заходим на клиентскую машину кластера. Клиентская машина -- это один компьютер, то есть никакой распредленности. Но на ней настроена работа с остальными машинами кластерами.

### Запуск программ MPI локально

_Инструкция ниже подходит для запуска на пользовательских машинах (например, через Docker ниже) и на клиентской машины кластера_

Для локального запуска можно использовать скрипт:
```bash
#!/bin/bash
mpiexec -np 4 ./a.out
```

Опция ```-np (-c|-n|-np|--np <arg0>)``` используется для указания количества процессов.

MPI локально может быть установлен для следующих ОС (на кластере уже есть):

Ubuntu: `sudo apt-get install openmpi-bin libopenmpi-dev`

Mac OS: `brew install open-mpi`

### Команды по использованию SLURM

SLURM -- планировщик (ресурсный менеджер кластера), установлен на кластере. Позволяет запустить задачу на всём кластере, а не на одной машине клиента.

* `sinfo` - посмотреть информацию по нодам кластера
* `sinfo -N -l` - посмотреть информацию по каждой ноде кластера
* `squeue` - посмотреть очередь задач
* `srun <command>` - запустить команду на ноде кластера
* `sbatch <script>` - запустить скрипт на нодах кластера. Каждый скрипт должен начинаться с `#!/bin/bash`.
  После этого должно высветиться сообщение `Submitted batch job <job_id>`, результаты работы попадают в лог-файл `slurm-<job_id>.out`.

### Запуск программ MPI на кластере

Пробуем запустить задачу на всём кластере, для этого понадобится команда `sbatch`. Готовим файл `run.sh`:

```bash
#!/bin/bash
sbcast -f a.out a.out #Копирование a.out на остальные ноды для запуска
mpiexec ./a.out
```

Отправляем задачу в очередь на Slurm:
```bash
sbatch -n 4 --comment="Hello" run.sh
```

На выходе получили что-то вроде Submitted batch job 123. Это означает, что ваша программа отправлена в очередь на запуск. Так как программа маленькая, то уже сразу можно в текущей директории (запуска) просмотреть результат. ls Там вы увидите файлы по типу slurm-123.out. В них и будет записан вывод, полученный от всех процессов (вместе со всеми ошибками).
Совет: все задачи делать в папке вашего пользователя, не создавая подпапки, так как система не стабильно работает со сложными иерархиями. И не забывайте пожалуйста про sbcast, иначе сложно будет запускать программв без копироавания файлов на остальные сервера....

Можно попробовать добавить настройки запуска джобы:

```bash
#!/bin/bash

#SBATCH --ntasks=4 # суммарное кол-во процессов
#SBATCH --ntasks-per-node=2 # Кол-во задач на процессор/машину
#SBATCH --job-name=PD-12345Task # Имя задачи для очереди
#SBATCH --comment="Run student mpi from config" # Крайне обязательный пункт, без него придёт автокил
#SBATCH --output=out.txt # Файл для печати вывода 
#SBATCH --error=error.txt # Файл для печати ошибок
sbcast -f a.out a.out
mpiexec ./a.out
```

Попробуем запустить:
```bash
sbatch run.sh
```

По итогу вы получите 2 файла - out.txt и error.txt, вдоволь наполненные ошибками. Но среди них можно найти вывод программ и увидеть именно то, чего мы и добивались - наша программа запустилась на 2 разных нодах, на каждой по 2 раза. В сумме 4 процессов. Победа)

### Рекомендации

Данный run.sh в некотором смысле универсален. Советую его переделать под себя 1 раз (имена файлов, названия job под своё имя пользователя) и уже через него отправлять отлаженные задачи в очередь. Кстати, свою очередь задач можете посмотреть через команду `sacct`. Глобальная очередь текущих задач доступна через команду `squeue`.

Пара советов по работе - для дебага и написания задачи крайне желательно либо запускать её локально на своём компьютере, либо на 2-3 процессах локально на клиенте. И лишь в последней итерации тестирования кода можете приступать к запуску кода через sbatch для получения результатов.

Само д/з вам точно придётся делать через кластер с помощью `sbatch`, так как требуется измерить ускорение работы программы против издержек передачи данных по сети между нодами.

### Локальный запуск программ через Docker:

Создаём файл run_docker.sh:
```
CONTAINER_NAME=pd-mpi

docker run -d -e TZ=Europe/Moscow -e OMPI_ALLOW_RUN_AS_ROOT=1 -e OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1 \
  --volume=`pwd`:/home --name="${CONTAINER_NAME}" ubuntu:18.04 tail -f /dev/null
docker exec -t "${CONTAINER_NAME}" apt update
docker exec -t "${CONTAINER_NAME}" apt upgrade -y
docker exec -t "${CONTAINER_NAME}" apt-get install -y \
  build-essential make vim g++ sudo libomp-dev cmake libopenmpi-dev \
  openmpi-common openmpi-bin libopenmpi-dev openssh-client openssh-server net-tools netcat iptables
```

Запускаем `bash run_docker.sh`, поднимается контейнер:

<img width="1382" alt="image" src="https://user-images.githubusercontent.com/36137274/190561579-356e8c4f-9662-423e-bb4b-7377556b608d.png">

Подключаемся к контейнеру командой:

`docker exec -it pd-mpi /bin/bash`

#### Запуск программ

1. `mpic++ main.cpp `
2. `mpiexec -np 4 --allow-run-as-root ./a.out`

#### Запуск программ с make

1. `cmake . `
2. `make -j9`
3. `mpiexec -n 4 --allow-run-as-root bin/..`


#### Репозиторий с кодом:
https://github.com/akhtyamovpavel/ParallelComputationExamples/tree/master/MPI

### Полезные ссылки:
* [Mануал OpenMPI 4.0](https://www.open-mpi.org/doc/current/)
* [MPI для начинающих](https://www.opennet.ru/docs/RUS/MPI_intro/)
* [Tutorial по MPI с примерами](https://mpitutorial.com)
* [Примеры работы с sbatch в bash](https://hpc-uit.readthedocs.io/en/latest/jobs/examples.html)
* [Текст про slurm](https://parallel.uran.ru/book/export/html/547)
* [Python-клиент для работы с sbatch](https://github.com/luptior/pysbatch)
* [Python-библиотека для разработки MPI программ](https://mpi4py.readthedocs.io/en/stable/intro.html)
